{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b33e437e",
   "metadata": {},
   "source": [
    "# Implementing Multiple Linear Regression\n",
    "\n",
    "We are now ready to actually implement a multiple regression model from scratch using Python!\n",
    "\n",
    "As we did in univariate linear regression, we'll start by importing two libraries: `numpy` for handling matrix computations, and `pandas` for importing, exporting and visualizing our data.\n",
    "\n",
    "Recall our importing syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc7f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b43cb3",
   "metadata": {},
   "source": [
    "We'll now use `pandas` to read our apartments price dataset into a `pandas` dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aea107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/marciorvneto/ml-course/main/multivariate-linear-regression/apartments.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e1fc7",
   "metadata": {},
   "source": [
    "## Visualizing our data using scatter plots\n",
    "\n",
    "It is always a good idea to get a sense of our data before attempting to train any models on it. For high-dimensional data, a nice way of visualizing pairwise relations between features is by using a scatter plot matrix. Pandas makes this very simple for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbbc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a00aff",
   "metadata": {},
   "source": [
    "## Splitting our data into training and testing datasets\n",
    "\n",
    "As we've discussed before, we should split our dataset into a training dataset and a testing dataset. We fit our model with the training data, and evaluate its performance using data that it's never seen before - the test data.\n",
    "\n",
    "We'll proceed as before, by doing a 70%/30% split on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_training = 0.7\n",
    "percent_test = 1 - percent_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d56dc97",
   "metadata": {},
   "source": [
    "As we've mentioned in our last demonstration, it's good practice to shuffle our training data to break possible sequential correlations between our data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total_data = len(df)                                  # Total number of data points\n",
    "num_training_data = int(percent_training*num_total_data)  # Number of training data points\n",
    "num_test_data = num_total_data - num_training_data        # Number of test data points\n",
    "\n",
    "indices = np.arange(num_total_data)                       # Create an array of indices for our data\n",
    "np.random.shuffle(indices)                                # Shuffle the indices\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8e20b",
   "metadata": {},
   "source": [
    "Notice that the indices have been permuted. Now, all we've got left to do is select the first `num_training_data` indices as our training indices, and select the remaining ones to be our test indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ff0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices = indices[:num_training_data]\n",
    "test_indices = indices[num_training_data:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339a06a",
   "metadata": {},
   "source": [
    "# Building the design matrix\n",
    "\n",
    "Recall that in order to find the multiple linear regression coefficients $\\hat{\\beta}$, we need to solve the following system of equations, also known as the *normal equations*:\n",
    "\n",
    "$$(\\mathbb{X}^T\\mathbb{X})\\hat{\\beta} = \\mathbb{X}^T\\mathbb{Y}$$\n",
    "\n",
    "The matrix $\\mathbb{X}$ is the so-called *design matrix*. Recall that its first column is made of ones, and each subsequent column corresponds to a feature of our model.\n",
    "\n",
    "Numpy and pandas make creating this matrix quite straightforward. We first convert our pandas dataframe to a numpy array `dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebdeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d11886",
   "metadata": {},
   "source": [
    "We can now separate our training data from our test data by taking the corresponding rows from `dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataset[training_indices]\n",
    "test_data = dataset[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336009a7",
   "metadata": {},
   "source": [
    "`training_data` and `test_data` are two numpy matrices containing our train and test data. Now let's create the design matrix using our training data.\n",
    "\n",
    "The first thing we need is a column of ones. Numpy's function `ones` allows us to create an array of ones of whichever shape we like. We need a column of ones with `num_training_data` elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c88f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ones = np.ones((num_training_data,1))\n",
    "col_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79875b0f",
   "metadata": {},
   "source": [
    "The next columns correspond to our features. By inspecting our data, we notice that the first and second columns (indices 0 and 1, in Python talk) correspond to area and distance. The third column (index 2), on the other hand, corresponds to the price, our label.\n",
    "\n",
    "Therefore, we only need to include columns from 0 to 2 (not including). In Python, we can isolate them like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126c4e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = training_data[:, 0:2]   # 0:2 means 0 and 1, 2 is not included\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23768cdd",
   "metadata": {},
   "source": [
    "Now we need to **horizontally stack** these columns to complete our design matrix. We do that using the funciton `hstack`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7757278d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.hstack([col_ones, features])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f91ae8",
   "metadata": {},
   "source": [
    "Nice! Let us now take the last column of our training data to get our prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5a5e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y = training_data[:,-1]    # This is equivalent to training_data[:,2]. \"-1\" means \"last index\"\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab2978",
   "metadata": {},
   "source": [
    "## Solving for the model parameters\n",
    "\n",
    "All we need to do now is solve the linear system corresponding to the normal equations.\n",
    "\n",
    "We're in luck, because `numpy` is excellent at solving linear systems! In fact, we can solve very large systems very quickly and with great precision.\n",
    "\n",
    "In order to leverage `numpy`'s system solving capabilities, we need to express our linear system as:\n",
    "\n",
    "$$ A\\mathbf{x} = \\mathbf{b} $$\n",
    "\n",
    "Where $A$ is a matrix of constant coefficients, $\\mathbf{b}$ is a vector (or column matrix) of constants, and $\\mathbf{x}$ is our vector of unknowns.\n",
    "\n",
    "Let's compare this equation with the normal equations:\n",
    "\n",
    "$$(\\mathbb{X}^T\\mathbb{X})\\hat{\\beta} = \\mathbb{X}^T\\mathbb{Y}$$\n",
    "\n",
    "Our vector of unknowns here are the $\\hat{\\beta}$. Likewise, we find that:\n",
    "\n",
    "* $A = \\mathbb{X}^T\\mathbb{X}$\n",
    "* $\\mathbf{b} = \\mathbb{X}^T\\mathbb{Y}$\n",
    "\n",
    "Let's see how we can build  $A$ and $\\mathbf{b}$ using `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08939782",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X.T.dot(X)           # Transposes X and multiplies it by X\n",
    "b = X.T.dot(Y)           # Transposes X and multiplies it by Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9508a",
   "metadata": {},
   "source": [
    "Solving the system for $\\hat{\\beta}$ is now as simple as calling a function. To be more precise, we'll be calling the function `solve`, which is stored in the linear algebra module `linalg` of `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0905cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat = np.linalg.solve(X.T.dot(X), X.T.dot(Y))\n",
    "beta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f61514",
   "metadata": {},
   "source": [
    "## Evaluating our model's performance\n",
    "\n",
    "As before, in order to evaluate how well our model should perform on real-world data, we'll calculate the mean squared error, MSE, using our test data, which we expect to be a good proxy to real world data.\n",
    "\n",
    "Let us begin by defining a function `mse`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(beta, x, y_real):\n",
    "    y_predicted = x.dot(beta)\n",
    "    error = np.mean((y_real - y_predicted)**2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b131951",
   "metadata": {},
   "source": [
    "Let's see how well we're doing when we plug in our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92741482",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.hstack([np.ones((num_test_data,1)), test_data[:,0:2]])\n",
    "Y_test = test_data[:,-1]\n",
    "\n",
    "test_error = mse(beta_hat, X_test, Y_test)\n",
    "test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97428e8",
   "metadata": {},
   "source": [
    "## Graphically visualizing our model's quality\n",
    "\n",
    "It's nice to get a sense of how well our model behaves in the test data by plotting the predicted prices versus their actual values.\n",
    "\n",
    "If our predictions were perfect, all our values would line up along the line $y = x$. In practice, however, this is not the case. The more our predicted points deviate from this line, the worse our predictions are.\n",
    "\n",
    "Let's see how we can visualize our data using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e35be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Y_predicted = X_test.dot(beta_hat)\n",
    "\n",
    "plt.plot(Y_test, Y_predicted, 'o')\n",
    "x = np.linspace(np.min(Y_test), np.max(Y_test))\n",
    "\n",
    "plt.plot(x,x)\n",
    "plt.grid()\n",
    "plt.legend([\"Predicted values\", \"y=x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934d5a1",
   "metadata": {},
   "source": [
    "Not bad at all, but there's room for improvement. In the next lectures, we'll see how we can leverage our "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
